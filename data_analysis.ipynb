{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os, json\n",
    "from json import JSONDecodeError\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(from_year, to_year):\n",
    "    if not 2000 < int(from_year) < 2050 or not 2000 < int(from_year) < 2050:\n",
    "        return []\n",
    "    all_holidays = []\n",
    "    for i in range(from_year, to_year+1):\n",
    "        r = requests.get(f'https://date.nager.at/api/v2/publicholidays/{i}/CH')\n",
    "        all_holidays.extend(r.json())\n",
    "    return all_holidays\n",
    "\n",
    "def set_date_index(df, col='date'):\n",
    "    return df.set_index(pd.to_datetime(df[col])).drop(col, axis=1)\n",
    "\n",
    "def filter_canton(df, can='BS'):\n",
    "    return df[[(str('CH-' + can) in row) if row is not None else True for row in df.counties]]\n",
    "\n",
    "def get_holiday_data(canton='BS', from_year=2015, to_year=2019):\n",
    "    holiday_data = get_json(from_year, to_year)\n",
    "    holiday_df = (pd.DataFrame.from_records(holiday_data)\n",
    "                  .pipe(set_date_index)\n",
    "                  .pipe(filter_canton, can=canton))\n",
    "    return holiday_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 105: Basel\n",
    "def import_weather_data(station_index=105):    \n",
    "    stationdata = []\n",
    "    station_error = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(\"weather_data/2018\"):\n",
    "        for file in files: \n",
    "            if not file.endswith(\".json\"): continue\n",
    "            filename = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(filename) as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    stationdata.append(data[station_index])\n",
    "            except UnicodeDecodeError:\n",
    "                station_error = station_error + 1\n",
    "            except JSONDecodeError:\n",
    "                station_error = station_error + 1\n",
    "    print (\"Loaded: %d - errors: %d\" % (len(stationdata), station_error))\n",
    "    pandata = pd.DataFrame(stationdata)\n",
    "    pandata = pandata.drop(columns=['code','station'])\n",
    "    return pandata\n",
    "\n",
    "def clean_weather_data(weather_df): \n",
    "    weather_df['temperature'] = pd.to_numeric(weather_df['temperature'])\n",
    "    weather_df['sunshine'] = pd.to_numeric(weather_df['sunshine'])\n",
    "    weather_df['precipitation'] = pd.to_numeric(weather_df['precipitation'])\n",
    "    return weather_df.set_index(pd.to_datetime(weather_df['dateTime'])).drop('dateTime', axis=1).sort_index()\n",
    "\n",
    "def get_weather_score():\n",
    "    weather_data = (import_weather_data()\n",
    "                    .pipe(clean_weather_data))\n",
    "    # first version, just take sunshine value\n",
    "    return weather_data[['sunshine']]\n",
    "\n",
    "def get_metheo_weather():\n",
    "    metheo = pd.read_csv('raw_data/metheo.csv', delimiter=';')\n",
    "    metheo['Date'] = pd.to_datetime(metheo[['Year','Month','Day','Hour','Minute']])\n",
    "    metheo = metheo.drop(['Year','Month','Day','Hour','Minute'], axis=1).set_index('Date')\n",
    "    metheo['SWR'] = metheo.pop('Shortwave Radiation')\n",
    "    return metheo\n",
    "\n",
    "def get_basel_kanton_weather():\n",
    "    basel_wetter = pd.read_csv('raw_data/100051.csv', delimiter=';', parse_dates=True)\n",
    "    basel_wetter.set_index(pd.to_datetime(basel_wetter['Datum/Zeit'])).drop('Datum/Zeit', axis=1).sort_index().dropna()\n",
    "    return basel_wetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_basel_data():\n",
    "    data = pd.read_csv(\"raw_data/Basel_Daten.csv\", delimiter=';')\n",
    "    stamps = [x.split('+')[0] for x in data['DateTimeFrom'].values]\n",
    "    data = data.assign(Time=pd.to_datetime(stamps.copy(), format=\"%Y-%m-%dT%H:%M:%S\"))\n",
    "    return data\n",
    "\n",
    "def clean_location_data(location_df):\n",
    "    location_df = location_df.set_index('Time')\n",
    "    location_df = location_df.assign(Day=location_df.index.day)\n",
    "    return location_df\n",
    "\n",
    "def get_location_data(df, location):\n",
    "    mask = df['SiteName'] == location\n",
    "    return df[mask], df[~mask]\n",
    "\n",
    "def get_group_data(df, group):\n",
    "    mask = df['Group'] == group\n",
    "    return df[mask], df[~mask]\n",
    "\n",
    "def get_location_split_dict(data):\n",
    "    location_dict = {}\n",
    "    #Alle Orte\n",
    "    locs = sorted(list(set(data['SiteName'])))\n",
    "    for loc in locs:\n",
    "        location_df, data = get_location_data(data, loc)\n",
    "        location_dict[loc] = clean_location_data(location_df)\n",
    "    return location_dict\n",
    "\n",
    "def get_group_split_dict(data):\n",
    "    group_dict = {}\n",
    "    location_group = {'350 Dreirosenbrücke': 3, '352 Johanniterbrücke': 3, '354 Wettsteinbrücke': 3,\n",
    "                      '403 Heuwaage-Viadukt': 3, '405 Dorenbachviadukt': 2, '659 Schlachthofstrasse': 3,\n",
    "                      '660 Flughafenstrasse': 3, '802 Klybeckstrasse 113/Kirche': 1, '803 Johanniterbrücke': 1,\n",
    "                      '804 Rosentalstrasse 29/28': 2, '805 Rebgasse 11/28': 1, '806 Gerbergasse': 1,\n",
    "                      '807 Güterstrasse 180/183': 1, '808 Wolfschlucht-Promenade': 1, '809 Allschwilerstrasse 77/86': 1,\n",
    "                      '810 Neubadstrasse 124/137': 1, '811 Mülhauserstrasse 110/122': 1, '812 Wettsteinbrücke': 1,\n",
    "                      '813 Hardstrasse 66/77': 1, '814 Elisabethenstrasse 46*': 1, '815 Mittlere Rheinbrücke': 1,\n",
    "                      '816 Schmiedgasse 4/7 (Riehen)': 1, '817 Elisabethenanlage': 1, '901 Peter-Merian Weg': 3,\n",
    "                      '902 Viaduktstrasse': 3, '903 Äussere Baselstrasse 328': 2, '904 Hammerstrasse 90': 2,\n",
    "                      '905 Leimenstrasse 4': 2, '906 Hegenheimerstrasse 44': 2, '907 Wasgenring 62': 2,\n",
    "                      '908 Grenzacherstrasse (Kraftwerk)': 2, '909 General Guisan-Strasse 104': 2,\n",
    "                      '910 St. Galler-Ring 101': 2, '911 Birskopfsteg': 2, '912 Elsässerstrasse 261/260': 2,\n",
    "                      '913 Burgfelderstrasse': 3, '914 Hiltalingerstrasse': 1, '915 Luzernerring-Brücke': 3,\n",
    "                      '916 Stückisteg': 1, '917 Schwarzwaldbrücke': 3, '918 Elsässerrheinweg': 0,\n",
    "                      '919 St. Alban-Rheinweg': 2, '920 J. Burckhardt-Strasse': 3}\n",
    "    data['Group'] = data['SiteName'].apply(lambda x: location_group[x])\n",
    "    for group in range(4):\n",
    "        group_df, data = get_group_data(data, group)\n",
    "        group_df = group_df.assign(Day=group_df['Time'].dt.day)\n",
    "        group_dict[group] = group_df\n",
    "    return group_dict\n",
    "\n",
    "#TODO CREATE REAL SAMPLES\n",
    "def resample_location_data(location_df, frequency):\n",
    "    if frequency == 'D':\n",
    "        cols = ['Total']\n",
    "    elif frequency == 'H':\n",
    "        cols = ['Total', 'Month', 'Day', 'Holiday', 'Weekday', 'HourFrom', 'Temperature', 'Precipitation', 'SWR']\n",
    "    else:\n",
    "        print(\"INCORRECT FREQUENCY\")\n",
    "    location_df = location_df[cols].resample(frequency).sum()\n",
    "    return location_df\n",
    "\n",
    "def cyclical_encoding(x, m, f):\n",
    "    return f(2*math.pi*x/m)\n",
    "\n",
    "def create_classification_samples(data, group):\n",
    "    cols = ['Total', 'Month', 'Day', 'Holiday', 'Weekday', 'HourFrom', 'Temperature', 'Precipitation', 'SWR', 'SiteName', 'Time']\n",
    "    data = data[cols]\n",
    "    data['SinMonth'] = data['Month'].apply(cyclical_encoding, m=12, f=math.sin)\n",
    "    data['CosMonth'] = data['Month'].apply(cyclical_encoding, m=12, f=math.cos)\n",
    "    data['SinDay'] = data['Day'].apply(cyclical_encoding, m=30, f=math.sin)\n",
    "    data['CosDay'] = data['Day'].apply(cyclical_encoding, m=30, f=math.cos)\n",
    "    data['SinWeekday'] = data['Weekday'].apply(cyclical_encoding, m=7, f=math.sin)\n",
    "    data['CosWeekday'] = data['Weekday'].apply(cyclical_encoding, m=7, f=math.cos)\n",
    "    #data['SinHour'] = data['Hour'].apply(cyclical_encoding, m=24, f=math.sin)\n",
    "    #data['CosHour'] = data['Hour'].apply(cyclical_encoding, m=24, f=math.cos)\n",
    "    data = data.drop(['Month', 'Day', 'Weekday'], axis=1)\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    X = data[data['HourFrom'].isin([7, 12, 15])]\n",
    "    for loc in list(set(X['SiteName'])):\n",
    "        loc_mask = X['SiteName'] == loc\n",
    "        for day in list(set(X['Time'].dt.date)):\n",
    "            day_mask = X['Time'].dt.date == day\n",
    "            full_mask = (loc_mask) & (day_mask)\n",
    "            samples = X[full_mask]\n",
    "            #print(samples)\n",
    "            X = X[~full_mask]\n",
    "            X_filtered = samples[['Total', 'SinMonth', 'CosMonth', 'SinDay', 'CosDay', 'SinWeekday', 'CosWeekday',\n",
    "                                  'Holiday', 'Precipitation', 'SWR']]\n",
    "            total_sum = X_filtered['Total'].sum()\n",
    "            X_filtered['Total'] = X_filtered['Total'] / total_sum\n",
    "            #print(X_filtered)\n",
    "            #print(X_filtered.values)\n",
    "            if not X_filtered.empty:\n",
    "                if random.random() > 0.75:\n",
    "                    X_test.append(X_filtered.values)\n",
    "                else:\n",
    "                    X_train.append(X_filtered.values)\n",
    "    y_train = [group] * len(X_train)\n",
    "    y_test = [group] * len(X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df = get_holiday_data()\n",
    "#weather_data = (import_weather_data()\n",
    "#                .pipe(clean_weather_data))[['sunshine', 'precipitation', 'temperature']]\n",
    "metheo = get_metheo_weather()\n",
    "data = import_basel_data()\n",
    "data = data.drop('SiteCode', axis=1)\n",
    "data['Holiday'] = [1 if x in holiday_df.index else 0 for x in data['Time']]\n",
    "data = pd.merge(data, metheo, left_on='Time', right_index=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:87: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/home/georg/anaconda3/envs/ds/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "X_c_train, X_c_test, y_c_train, y_c_test = [], [], [], []\n",
    "for group, group_data in get_group_split_dict(data).items():\n",
    "    X_train, X_test, y_train, y_test = create_classification_samples(group_data.copy(), group)\n",
    "    X_c_train.extend(X_train)\n",
    "    X_c_test.extend(X_test)\n",
    "    y_c_train.extend(y_train)\n",
    "    y_c_test.extend(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_train = [x.reshape(30) for x in X_c_train]\n",
    "Input_test = [x.reshape(30) for x in X_c_test]\n",
    "Input_train = np.array(Input_train)\n",
    "Input_test = np.array(Input_test)\n",
    "y_c_train = np.array(y_c_train)\n",
    "y_c_test = np.array(y_c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(Input_train, y_c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(Input_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_c_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.30%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_c_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds] *",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
